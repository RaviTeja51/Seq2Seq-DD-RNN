{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eH5pcthcbHZZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "%cd /content/gdrive/My\\ Drive/Seq2Seq\\-DD\\-RNN\n",
    "\n",
    "\n",
    "sys.path.append('/content/gdrive/My Drive/Seq2Seq-DD-RNN/attention.py')\n",
    "from attention import LuongAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpJ7QWmD2n8F"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtdRFdAHbm43"
   },
   "outputs": [],
   "source": [
    "def char_index_map():\n",
    "  # sanskrit characters unicode range from 0x900 - 0x97F\n",
    "\n",
    "  # convert to integer representation \n",
    "  begin = int(\"0x900\",16)\n",
    "  end = int(\"0x97F\",16)\n",
    "\n",
    "  char_to_index  = {}\n",
    "  index = 1\n",
    "\n",
    "  ## there are 128 characters in sanskrit\n",
    "  ## values 129, 130, 131 are +, S,E\n",
    "\n",
    "  for i in range(begin, end+1):\n",
    "    char_to_index[chr(i)] = index \n",
    "    index += 1\n",
    "\n",
    "  char_to_index[\"+\"] = index \n",
    "  char_to_index[\"S\"] = index+1\n",
    "  char_to_index['E'] = index+2\n",
    "  \n",
    "  index_to_char = {} \n",
    "  val = begin\n",
    "\n",
    "  for i in range(1,129):\n",
    "    index_to_char[i] = chr(val)\n",
    "    val += 1\n",
    "\n",
    "  index_to_char[129] = \"+\"\n",
    "  index_to_char[130] = \"S\"\n",
    "  index_to_char[131] = \"E\"\n",
    "\n",
    "  return char_to_index, index_to_char\n",
    "\n",
    "\n",
    "\n",
    "def seq_length(text):\n",
    "    # find longest string length \n",
    "    max_len = -1\n",
    "    for i in range(text.shape[0]):\n",
    "      max_len = max(len(text[i][0]),max_len)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "840Bj-gNcsPp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qit6rs2f57Ft"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "\n",
    "  def __init__(self, out_dim,  \n",
    "               attention, embedded_layer=None, \n",
    "               hidden_units= 512, dropout=0.3):\n",
    "    # call base constructor from child\n",
    "    super(Decoder,self).__init__()\n",
    "\n",
    "    self.out_dim =  out_dim\n",
    "    self.embedded_layer = embedded_layer # share embedded layer.\n",
    "    self.latent_dims = hidden_units  \n",
    "    self.attention = attention  # attention layer.\n",
    "\n",
    "    self.lstm = layers.LSTM(hidden_units,\n",
    "                            return_sequences=True,\n",
    "                            return_state=True,\n",
    "                            dropout=0.3)\n",
    "    \n",
    "    self.wa = layers.Dense(hidden_units, activation=\"tanh\")\n",
    "    self.fc = layers.Dense(out_dim, activation = \"softmax\")\n",
    "\n",
    "  def __call__(self,x, states, encoder_outputs):\n",
    "    \n",
    "   \n",
    "    # encode, x shape(batch_size, 1)\n",
    "    x = self.embedded_layer(x)\n",
    "    \n",
    "    x = tf.expand_dims(x,1)\n",
    "    lstm_out,hidden_state, cell_state = self.lstm(x, initial_state = states)\n",
    "    \n",
    "    #lstm_out  shape (batch_size, 1, hidden_units)\n",
    "    #hidden_state, cell_state  shape (batch_size, hidden_units)\n",
    "  \n",
    "    # apply attention\n",
    "    context_vector, attention_weights = self.attention(hidden_state, encoder_outputs) \n",
    "    # context_vector is of shape (batch_size, time_stps, hidden_units)\n",
    "   \n",
    "    lstm_out = tf.concat([tf.squeeze(context_vector, 1), tf.squeeze(lstm_out, 1)], 1)\n",
    "\n",
    "\n",
    "    #lstm_out shape (batch_size, hidden_units)\n",
    "    lstm_out = self.wa(lstm_out)  #shape (batch_size, hidden_units)\n",
    "    \n",
    "\n",
    "    decoder_out = tf.expand_dims(self.fc(lstm_out),1)\n",
    "    #decoder_out  shape (batch_size, 1, out_dims)\n",
    "\n",
    "    return decoder_out, hidden_state, cell_state, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKH90pRFdLip"
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "\n",
    "  def __init__(self, embedded_layer, \n",
    "               hidden_units= 512, dropout=0.3):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.embedded_layer = embedded_layer\n",
    "    self.hidden_units = hidden_units\n",
    "\n",
    "    self.lstm1_forward = layers.LSTM(hidden_units,\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=dropout)\n",
    "    self.lstm1_backward = layers.LSTM(hidden_units,\n",
    "                                      return_sequences=True,\n",
    "                                      dropout=dropout,\n",
    "                                      go_backwards=True)\n",
    "    \n",
    "    self.layer1 = layers.Bidirectional(self.lstm1_forward,\n",
    "                                       backward_layer = self.lstm1_backward)   #1st layer\n",
    "    \n",
    "    self.lstm2_forward = layers.LSTM(hidden_units,\n",
    "                                     return_sequences=True,\n",
    "                                     return_state=True,\n",
    "                                     dropout=dropout)\n",
    "    \n",
    "    self.lstm2_backward = layers.LSTM(hidden_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      dropout=dropout,\n",
    "                                      go_backwards=True)\n",
    "    \n",
    "    self.layer2 = layers.Bidirectional(self.lstm2_forward,\n",
    "                                       backward_layer=self.lstm2_backward,\n",
    "                                       merge_mode=\"sum\")     #2nd layer\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    x = self.embedded_layer(x)\n",
    "    x = self.layer1(x)  \n",
    "    # x shape is (batch_size, time_steps, 2*hidden_units)\n",
    "\n",
    "    encoder_output,forward_h, forward_c, backward_h, backward_c  = self.layer2(x)\n",
    "\n",
    "    hidden_state = forward_h + backward_h \n",
    "    memory_state = forward_c + backward_c\n",
    "\n",
    "    encoder_states = [hidden_state, memory_state]\n",
    "\n",
    "    # encoder_output shape is (batch_size, time_steps, hidden_units)\n",
    "    # hidden_state, memory_state shape is (batch_size, hidden_units)\n",
    "\n",
    "    return encoder_output, encoder_states\n",
    "  \n",
    "  def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.hidden_units]),\n",
    "                tf.zeros([batch_size, self.hidden_units]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTKavuqB4qNH"
   },
   "outputs": [],
   "source": [
    "def load_txt():\n",
    "  PATH = '/content/gdrive/My Drive/Seq2Seq-DD-RNN/sanskrit_sandhi_corpus-master'\n",
    "\n",
    "  inp = np.loadtxt(PATH+\"/combined_inp.txt\", dtype=\"object\").reshape(-1,1)\n",
    "\n",
    "  with open(PATH+\"/combined_out.txt\") as f:\n",
    "    out = f.readlines()\n",
    "  \n",
    "  with open(PATH+\"/sandhi_pos.txt\") as f:\n",
    "    sandhi = f.readlines()\n",
    "  \n",
    "  examples = len(out)\n",
    "  for i in range(examples):\n",
    "    out[i] = \"S\"+out[i].strip(\"\\n\")+\"E\"\n",
    "  \n",
    "  for i in range(examples):\n",
    "    sandhi[i] = sandhi[i].strip(\"\\n\")\n",
    "\n",
    "  out = np.array(out, dtype=\"object\").reshape(-1,1)\n",
    "  sandhi_pos = np.array(sandhi, dtype =\"object\") .reshape(-1,1) \n",
    "\n",
    "  return inp,out,sandhi_pos, examples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVsSrNsXc4F9"
   },
   "outputs": [],
   "source": [
    "def load_data_split_gen(input_data, out_data, \n",
    "                        enc_len, decd_len,\n",
    "                        vocab_size, batch_size=64):\n",
    "  ## a generator for generating input for encoder and input, output for decoder to predict final sequence.\n",
    "  ## input_data - encoder input shape (examples,1)\n",
    "  ## out_data -   decoder output  shape (examples,1)\n",
    "  ## enc_len -  encoder longest sequence length\n",
    "  ## decd_len - decoder longest sequence length\n",
    "  \n",
    "  examples = input_data.shape[0]\n",
    "  steps = examples//batch_size #number of batches\n",
    "  char_to_index, index_to_char = char_index_map() \n",
    "\n",
    "  while True:\n",
    "\n",
    "    for i in range(steps):\n",
    "      index = [j for j in range(i*batch_size, (i+1)*batch_size)]\n",
    "\n",
    "      np.random.shuffle(index)\n",
    "      temp_inp = input_data[index]\n",
    "      temp_size = len(index)\n",
    "\n",
    "\n",
    "      # encoder input is a 2D matrix  of shape (number of examples, enc_len)\n",
    "      encoder_inp = np.zeros((temp_size,enc_len),dtype=int)\n",
    "\n",
    "      for i in range(temp_size):\n",
    "        m = len(temp_inp[i][0])\n",
    "        for j in range(m):\n",
    "          if m >= enc_len:\n",
    "             break\n",
    "          encoder_inp[i,j] = char_to_index[temp_inp[i][0][j]]\n",
    "        \n",
    "      # decoder input is a  2D matrix of shape (number of examples, decoder length)\n",
    "      decoder_inp = np.zeros((temp_size, dec_len),dtype=int)\n",
    "\n",
    "      # decoder output is a 3D matrix of shape( number of examples, decoder length, vocab size)\n",
    "      decoder_out = np.zeros((temp_size, dec_len, vocab_size))\n",
    " \n",
    "      temp_out =  out_data[index]\n",
    "      for i in range(temp_size):\n",
    "        m = len(temp_out[i][0])\n",
    "        for j in  range(m):\n",
    "          if m >= enc_len:\n",
    "             break\n",
    "          decoder_inp[i,j] = char_to_index[temp_out[i][0][j]]\n",
    "\n",
    "          # decoder output is one time step ahead of decoder input\n",
    "          if j > 0:\n",
    "              decoder_out[i,j-1,char_to_index[temp_out[i][0][j]]-1] = 1\n",
    "      \n",
    "      yield [encoder_inp, decoder_inp], decoder_out\n",
    "    \n",
    "    #shuffle the data\n",
    "    permute = np.random.permutation(examples)\n",
    "    input_data = input_data[permute]\n",
    "    out_data =  out_data[permute]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icRNsghX6XjS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FuW6TrlFYCV"
   },
   "outputs": [],
   "source": [
    "def gen_sandhi_pos_data(input_data, out_data,\n",
    "                        output, enc_len, \n",
    "                        decd_len, vocab_size, \n",
    "                        batch_size=64):\n",
    "\n",
    "  # a generator for generating input for encoder and input, output for decoder to predict sandhi split position.\n",
    "  # input_data - encoder input shape (examples,1)\n",
    "  # out_data -   decoder output of sandhi position shape(examples,1)\n",
    "  # output - sandhi splitted examples shape (examples, 1)\n",
    "  # enc_len -  encoder longest sequence length\n",
    "  # decd_len - decoder longest sequence length\n",
    "\n",
    "  examples = input_data.shape[0]\n",
    "  steps = examples//batch_size #number of batches\n",
    "  char_to_index, index_to_char = char_index_map() \n",
    "\n",
    "  while True:\n",
    "    for i in range(steps):\n",
    "      index = [j for j in range(i*batch_size, (i+1)*batch_size)]\n",
    "\n",
    "      np.random.shuffle(index)\n",
    "      temp_inp = input_data[index]\n",
    "      out = output[index]\n",
    "      temp_size = len(index)\n",
    "\n",
    "       # encoder input is a 2D matrix  of shape (number of examples, enc_len)\n",
    "      encoder_inp = np.zeros((temp_size,enc_len),dtype=int)\n",
    "\n",
    "      for i in range(temp_size):\n",
    "        m = len(temp_inp[i][0])\n",
    "        for j in range(m):\n",
    "          if m >= enc_len:\n",
    "             break\n",
    "          encoder_inp[i,j] = char_to_index[temp_inp[i][0][j]]\n",
    "      \n",
    "      # decoder input is a  2D matrix of shape (number of examples, decoder length)\n",
    "      decoder_inp = np.zeros((temp_size, decd_len),dtype=int)\n",
    "\n",
    "      # decoder output is a 3D matrix of shape(number of examples, decoder length, vocab size)\n",
    "      decoder_out = np.zeros((temp_size, decd_len, vocab_size),dtype=int)# vocab size is 4\n",
    "\n",
    "      temp_out =  out_data[index]\n",
    "      for i in range(temp_size):\n",
    "        \n",
    "\n",
    "        \n",
    "        n = len(out[i][0]) #sandhi splitted output length\n",
    "        decoder_inp[i][list(range(n+1))]=1\n",
    "        decoder_inp[i][0] = 3 # add start token\n",
    "        decoder_inp[i][n] = 4 # add end token\n",
    "        if temp_out[i][0] == \"\":\n",
    "          m = 0\n",
    "        else:\n",
    "          temp = list(map(int,temp_out[i][0].split(\",\")))\n",
    "          m = len(temp)\n",
    "\n",
    "        for j in  range(m):\n",
    "          if m >= enc_len:\n",
    "            break\n",
    "          decoder_inp[i][int(temp[j])+1] = 2 \n",
    "\n",
    "          # decoder output is one time step ahead of decoder input\n",
    "         \n",
    "          decoder_out[i,int(temp[j]),1] = 1\n",
    "        decoder_out[i,n-1,3] = 1\n",
    "      \n",
    "      yield encoder_inp, decoder_inp, decoder_out\n",
    "    \n",
    "    #shuffle the data\n",
    "    permute = np.random.permutation(examples)\n",
    "    input_data = input_data[permute]\n",
    "    out_data =  out_data[permute]\n",
    "    out = out[permute]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anjk6zwF-Qfe"
   },
   "outputs": [],
   "source": [
    "def gen_val_sandhi_pos_data(input_data, out_data,\n",
    "                        output, enc_len, \n",
    "                        decd_len, vocab_size, \n",
    "                        batch_size=64):\n",
    "\n",
    "  # a generator for generating input for encoder and input, output for decoder to predict sandhi split position.\n",
    "  # input_data - encoder input shape (examples,1)\n",
    "  # out_data -   decoder output of sandhi position shape(examples,1)\n",
    "  # output - sandhi splitted examples shape (examples, 1)\n",
    "  # enc_len -  encoder longest sequence length\n",
    "  # decd_len - decoder longest sequence length\n",
    "\n",
    "  examples = input_data.shape[0]\n",
    "  steps = examples//batch_size #number of batches\n",
    "  char_to_index, index_to_char = char_index_map() \n",
    "\n",
    "  for i in range(steps):\n",
    "    index = [j for j in range(i*batch_size, (i+1)*batch_size)]\n",
    "\n",
    "    np.random.shuffle(index)\n",
    "    temp_inp = input_data[index]\n",
    "    out = output[index]\n",
    "    temp_size = len(index)\n",
    "\n",
    "      # encoder input is a 2D matrix  of shape (number of examples, enc_len)\n",
    "    encoder_inp = np.zeros((temp_size,enc_len),dtype=int)\n",
    "\n",
    "    for i in range(temp_size):\n",
    "      m = len(temp_inp[i][0])\n",
    "      for j in range(m):\n",
    "        if m >= enc_len:\n",
    "          break\n",
    "        encoder_inp[i,j] = char_to_index[temp_inp[i][0][j]]\n",
    "    \n",
    "    # decoder input is a  2D matrix of shape (number of examples, decoder length)\n",
    "    decoder_inp = np.zeros((temp_size, decd_len),dtype=int)\n",
    "\n",
    "    # decoder output is a 3D matrix of shape(number of examples, decoder length, vocab size)\n",
    "    decoder_out = np.zeros((temp_size, decd_len, vocab_size),dtype=int)# vocab size is 4\n",
    "\n",
    "    temp_out =  out_data[index]\n",
    "    for i in range(temp_size):\n",
    "      \n",
    "\n",
    "      \n",
    "      n = len(out[i][0]) #sandhi splitted output length\n",
    "      decoder_inp[i][list(range(n+1))]=1\n",
    "      decoder_inp[i][0] = 3 # add start token\n",
    "      decoder_inp[i][n] = 4 # add end token\n",
    "      if temp_out[i][0] == \"\":\n",
    "        m = 0\n",
    "      else:\n",
    "        temp = list(map(int,temp_out[i][0].split(\",\")))\n",
    "        m = len(temp)\n",
    "\n",
    "      for j in  range(m):\n",
    "        if m >= enc_len:\n",
    "          break\n",
    "        decoder_inp[i][int(temp[j])+1] = 2 \n",
    "\n",
    "        # decoder output is one time step ahead of decoder input\n",
    "        \n",
    "        decoder_out[i,int(temp[j]),1] = 1\n",
    "      decoder_out[i,n-1,3] = 1\n",
    "    \n",
    "    yield encoder_inp, decoder_inp, decoder_out\n",
    "  \n",
    "  #shuffle the data\n",
    "  permute = np.random.permutation(examples)\n",
    "  input_data = input_data[permute]\n",
    "  out_data =  out_data[permute]\n",
    "  out = out[permute]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1BDQXQw-x0s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dw3WLs-e-yhx"
   },
   "outputs": [],
   "source": [
    "def gen_valid_data_split(input_data, out_data, \n",
    "                               enc_len, decd_len,\n",
    "                               vocab_size, batch_size=64):\n",
    "  ## a generator for generating input for encoder and input, output for decoder to predict final sequence.\n",
    "  ## input_data - encoder input shape (examples,1)\n",
    "  ## out_data -   decoder output  shape (examples,1)\n",
    "  ## enc_len -  encoder longest sequence length\n",
    "  ## decd_len - decoder longest sequence length\n",
    "  \n",
    "  examples = input_data.shape[0]\n",
    "  steps = examples//batch_size #number of batches\n",
    "  char_to_index, index_to_char = char_index_map() \n",
    "\n",
    " \n",
    "  for i in range(steps):\n",
    "    index = [j for j in range(i*batch_size, (i+1)*batch_size)]\n",
    "\n",
    "    np.random.shuffle(index)\n",
    "    temp_inp = input_data[index]\n",
    "    temp_size = len(index)\n",
    "\n",
    "\n",
    "    # encoder input is a 2D matrix  of shape (number of examples, enc_len)\n",
    "    encoder_inp = np.zeros((temp_size,enc_len),dtype=int)\n",
    "\n",
    "    for i in range(temp_size):\n",
    "      m = len(temp_inp[i][0])\n",
    "      for j in range(m):\n",
    "        if m >= enc_len:\n",
    "          break\n",
    "        encoder_inp[i,j] = char_to_index[temp_inp[i][0][j]]\n",
    "      \n",
    "    # decoder input is a  2D matrix of shape (number of examples, decoder length)\n",
    "    decoder_inp = np.zeros((temp_size, dec_len),dtype=int)\n",
    "\n",
    "    # decoder output is a 3D matrix of shape( number of examples, decoder length, vocab size)\n",
    "    decoder_out = np.zeros((temp_size, dec_len, vocab_size))\n",
    "\n",
    "    temp_out =  out_data[index]\n",
    "    for i in range(temp_size):\n",
    "      m = len(temp_out[i][0])\n",
    "      for j in  range(m):\n",
    "        if m >= enc_len:\n",
    "          break\n",
    "        decoder_inp[i,j] = char_to_index[temp_out[i][0][j]]\n",
    "\n",
    "        # decoder output is one time step ahead of decoder input\n",
    "        if j > 0:\n",
    "            decoder_out[i,j-1,char_to_index[temp_out[i][0][j]]-1] = 1\n",
    "    \n",
    "    yield [encoder_inp, decoder_inp], decoder_out\n",
    "  \n",
    "  #shuffle the data\n",
    "  permute = np.random.permutation(examples)\n",
    "  input_data = input_data[permute]\n",
    "  out_data =  out_data[permute]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXCGFbwAgLvz"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(source_word, decoder_in,\n",
    "               decoder_out, \n",
    "               ecnoder,decoder, entropy):\n",
    "  loss = 0\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    en_outputs, enc_states =  encoder(source_word)\n",
    "    hidden_state, cell_state = enc_states\n",
    "\n",
    "    # iterate through each time step\n",
    "    for i in range(decoder_in.shape[1]):\n",
    "      \n",
    "      target_seq_in = decoder_in[:,i]\n",
    "      \n",
    "      states = [hidden_state, cell_state]\n",
    "      target_out, hidden_state, cell_state, attention_weights =  decoder(target_seq_in,\n",
    "                                                                         states, \n",
    "                                                                         en_outputs)\n",
    "      \n",
    "      #  accumulate loss for  each time step for the entire batch\n",
    "      decoder_outs = tf.expand_dims(decoder_out[:,i,:],1)\n",
    "      loss += entropy(decoder_outs, target_out)\n",
    "\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss/source_word.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EF42njG2_bOi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(source_word, decoder_in,\n",
    "               decoder_out,\n",
    "               ecnoder,decoder, entropy):\n",
    "  \n",
    "   loss = 0\n",
    "  \n",
    "   en_outputs, enc_states =  encoder(source_word)\n",
    "   hidden_state, cell_state = enc_states\n",
    "\n",
    "   # iterate through each time step\n",
    "   for i in range(decoder_in.shape[1]):\n",
    "    \n",
    "     target_seq_in = decoder_in[:,i]\n",
    "     \n",
    "     target_out, hidden_state, cell_state, attention_weights =  decoder(x, states, en_outputs)\n",
    "     \n",
    "     decoder_outs = tf.expand_dims(decoder_out[:,i,:],1)\n",
    "     #  accumulate loss for  each time step for the entire batch\n",
    "     loss += entropy(decoder_outs, target_out)\n",
    "  \n",
    "   return loss/source_word.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hrkkUXnvWof"
   },
   "outputs": [],
   "source": [
    "def train(train_data_gen, valid_data_gen, \n",
    "          encoder, decoder,            \n",
    "          entropy, optimizer,\n",
    "          steps_per_epoch, steps,\n",
    "          EPOCHS=15, BATCH_SIZE=64,\n",
    "          decay_factor = 0.5):\n",
    "\n",
    "  val_loss_epoch = []\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    \n",
    "    # access the current learning rate using the optimizer\n",
    "    lr = float(keras.backend.get_value(optimizer.learning_rate))\n",
    "\n",
    "    if epoch < 2:\n",
    "      print(\"Not enought vaues to compare\")\n",
    "         \n",
    "    else:\n",
    "      if val_loss_epoch[-2] <= val_loss_epoch[-1]:\n",
    "        lr = lr *(1/(1+(epoch*decay_factor)))\n",
    "\n",
    "        # set the new learning rate\n",
    "        keras.backend.set_value(optimizer.lr, lr)\n",
    "        \n",
    "      else:\n",
    "          print(\"Validation loss improved\")\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"\\n Start of epochs: %d\\n Learning Rate %.0f\"%(epoch,lr))\n",
    "    \n",
    "    loss_value = 0\n",
    "    i = 0\n",
    "    for in_data, decoder_in,decoder_out in train_data_gen:\n",
    "      \n",
    "      print(f\"Batch {i+1}\")\n",
    "      loss = train_step(in_data, decoder_in, \n",
    "                               decoder_out, \n",
    "                               encoder,decoder, entropy)\n",
    "      loss_value += loss\n",
    "      print(f\"Batch  {i+1} loss: {loss}\")\n",
    "      i += 1\n",
    "    \n",
    "    print(\"Avg. train loss: %d\"%(loss_value/steps_per_epoch))\n",
    "\n",
    "    val_loss = 0\n",
    "    for in_data, decoder_in, decoder_out in valid_data_gen:\n",
    "      \n",
    "      val_loss += test_step(in_data, decoder_in,\n",
    "                            decoder_out,\n",
    "                            encoder, decoder, entropy)\n",
    "    \n",
    "    \n",
    "    val_loss_epoch.append(val_loss/steps)\n",
    "    print(\"Avg. valid loss: %d\"%val_loss_epoch[-1])\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end-start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aw_PVdDsQUCh"
   },
   "outputs": [],
   "source": [
    "def train_valid_split(inp, out, sandhi, examples, train_size=0.8):\n",
    "\n",
    "    # get indices to split the data into train and  validation.\n",
    "    np.random.seed(42)\n",
    "    train_indices = set(np.random.choice(range(examples), int((0.8)*examples), replace = False).flatten().tolist())\n",
    "    validation_indices = list(set(range(examples)) - train_indices)\n",
    "    train_indices = list(train_indices)\n",
    "\n",
    "    #shuffle the entire data\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(examples)\n",
    "    inp = inp[indices]\n",
    "    out = out[indices]\n",
    "    sandhi = sandhi[indices]\n",
    "\n",
    "    inp_train = inp[train_indices]\n",
    "    inp_valid = inp[validation_indices]\n",
    "\n",
    "    sandhi_train = sandhi[train_indices]\n",
    "    sandhi_valid = sandhi[validation_indices]\n",
    "\n",
    "    out_train = sandhi[train_indices]\n",
    "    out_valid = sandhi[validation_indices]\n",
    "\n",
    "    return [(inp_train, sandhi_train, out_train), (inp_valid, sandhi_valid, out_valid)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEKpyJgnwxnc"
   },
   "outputs": [],
   "source": [
    "inp, out, sandhi, examples = load_txt()\n",
    "\n",
    "enc_len = 1411\n",
    "decd_len = 1400\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzApf90KxRCL"
   },
   "outputs": [],
   "source": [
    "data = train_valid_split(inp, out, sandhi, examples) #split the data into train and vlaidation set\n",
    "inp_train, sandhi_train, out_train = data[0]\n",
    "inp_valid, sandhi_valid, out_valid = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kQcpLv6yhJI"
   },
   "outputs": [],
   "source": [
    "train_data_gen = gen_sandhi_pos_data(inp_train, sandhi_train,\n",
    "                        out_train, enc_len, \n",
    "                        decd_len,4, \n",
    "                        batch_size=64)\n",
    "\n",
    "valid_data_gen = gen_val_sandhi_pos_data(inp_valid, sandhi_valid,\n",
    "                        out_valid, enc_len, \n",
    "                        decd_len, 4, \n",
    "                        batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "kw3VttMqh_k0",
    "outputId": "dab204e6-b5a3-45d2-afa5-162aecd151d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enought vaues to compare\n",
      "\n",
      " Start of epochs: 0\n",
      " Learning Rate 1\n",
      "Batch 1\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "entropy = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1.0)\n",
    "\n",
    "steps_per_epoch = inp_train.shape[0]//BATCH_SIZE\n",
    "steps = inp_valid.shape[0]//BATCH_SIZE\n",
    "\n",
    "vocab_size = 131 #vocabulary size\n",
    "out_dim = int(np.log2(vocab_size))\n",
    "hidden_units = 512\n",
    "\n",
    "embedded_layer = layers.Embedding(vocab_size+1, out_dim, mask_zero=True)\n",
    "pos_embedded_layer = layers.Embedding(5, 4, mask_zero=True)\n",
    "\n",
    "# create encoder\n",
    "encoder = Encoder(embedded_layer) \n",
    "\n",
    "attention = LuongAttention(hidden_units)\n",
    "\n",
    "position_decoder = Decoder(4, attention, pos_embedded_layer)\n",
    "\n",
    "train(train_data_gen, valid_data_gen, \n",
    "          encoder, position_decoder,            \n",
    "          entropy, optimizer,\n",
    "          steps_per_epoch, steps,\n",
    "          EPOCHS=15, BATCH_SIZE=64,\n",
    "          decay_factor = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZT0dWjeVqk8N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2Seq -DD-RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
